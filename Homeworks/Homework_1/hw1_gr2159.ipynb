{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, please design, train, and evaluate a MLP (Multi-Layer Perceptron, aka Neural Network whose layers are all Linear layers) on the FashinMNIST data set.\n",
    "The dataset can be downloaded via PyTorch, just like how I downloaded the CIFAR-10 dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction and Importing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the most fundamental packages/modules of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # a Tensor library like NumPy, with strong GPU support\n",
    "import torch.optim # functions related to optimization algorithms\n",
    "\n",
    "import torch.nn # a neural networks library deeply integrated with autograd designed for maximum flexibility\n",
    "import torch.nn.functional\n",
    "# nn and nn.functional works kinda similarly. If you are interested in their subtile difference, please check out this discussion https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597\n",
    "\n",
    "import torch.utils.data # utility functions such as DataLoader\n",
    "\n",
    "\n",
    "# import torch # * a Tensor library like NumPy, with strong GPU support\n",
    "# import torch.optim as optim # functions related to optimization algorithms\n",
    "\n",
    "# import torch.nn as nn # a neural networks library deeply integrated with autograd designed for maximum flexibility\n",
    "# import torch.nn.functional as F\n",
    "# # nn and nn.functional works kinda similarly. If you are interested in their subtile difference, please check out this discussion https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597\n",
    "\n",
    "# import torch.utils.data as data # utility functions such as DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch essentially does two things:\n",
    "- Manipulates the so-called tensor data structure on GPU, just like NumPy can manipulate ndarray on CPU.\n",
    "- Provides a automatic differentiation engine and some convenient helper functions for deep learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is a data structure that can be thought of as a generalization of a matrix. A grayscale image is a matrix, but a colored image with 3 channels can be thought of a tensor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we are using GPU. Computation will be very slow if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import some vision-related packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally some generic helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import cv2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets a seed for the random number generators used by the random module, the numpy library, and the PyTorch library. By setting a seed, the code ensures that the results of the random number generation will be deterministic and reproducible, meaning that each time the code is run, the same sequence of random numbers will be generated. This is useful for debugging and testing, as well as for reproducing experimental results.\n",
    "\n",
    "Additionally, the code sets the device to either the GPU (if available) or the CPU. The PyTorch library allows computations to be performed on either the GPU or the CPU, and the device to be used can be specified by setting the device variable.\n",
    "\n",
    "Finally, the code sets torch.backends.cudnn.deterministic to True. This flag controls the deterministic behavior of the cuDNN library, which is used by PyTorch for GPU acceleration. By setting this flag to True, the code ensures that the cuDNN library will produce deterministic results and further improves the reproducibility of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Pretrained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "from PIL import Image\n",
    "\n",
    "# * we use Resnet pre-trained weights to help initialize weights and make our classification task perform better\n",
    "resnet18_model = torchvision.models.resnet18(weights=True)\n",
    "resnet18_model = resnet18_model.to(device)\n",
    "resnet18_model.eval()\n",
    "\n",
    "def prepare_an_img_resnet(img):\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
    "  \n",
    "    img = Image.open(img)\n",
    "    img = preprocess(img)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egyptian cat 0.30802950263023376\n",
      "Siamese cat 0.16848227381706238\n",
      "Angora 0.12675005197525024\n",
      "tabby 0.07222902774810791\n",
      "hamper 0.04242419824004173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Since we are not training the model, the gradients for this operation are not needed and can be temporarily disabled using torch.no_grad().\n",
    "with torch.no_grad():\n",
    "    # ! get the tensor form of the image\n",
    "    output = resnet18_model(prepare_an_img_resnet('cat.jpg').to(device))\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "# Download ImageNet labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
    "\n",
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work on our own model, we will train a slight variation of a network called *AlexNet*. This is a landmark model in deep learning, and arguably kickstarted the current (and ongoing, and massive) wave of innovation in modern AI in 2012. AlexNet was the first real-world demonstration of a *deep* classifier that was trained end-to-end on data and that outperformed all other ML models thus far.\n",
    "\n",
    "We will train AlexNet using the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. The classes are: \n",
    "\n",
    "0\tT-shirt/top\n",
    "\n",
    "1\tTrouser\n",
    "\n",
    "2\tPullover\n",
    "\n",
    "3\tDress\n",
    "\n",
    "4\tCoat\n",
    "\n",
    "5\tSandal\n",
    "\n",
    "6\tShirt\n",
    "\n",
    "7\tSneaker\n",
    "\n",
    "8\tBag\n",
    "\n",
    "9\tAnkle boot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is called finetuning. Please take a look at this [article](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset is included in PyTorch because it's so widely used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '.data' # folder that contains the \n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will do data augmentation. DL models are data hungry. A good trick to increse the size of dataset without the hardwork of acquiring/labeling more data is data augmentation. \n",
    "\n",
    "For each training image we will randomly rotate it (by up to 5 degrees), flip/mirror with probability 0.5, shift by +/-1 pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we compose all the data augmentation actions we want to do.\n",
    "# note that there is no need to do data augmentation on the testing set.\n",
    "train_transforms = [torchvision.transforms.RandomRotation(5),\n",
    "                  torchvision.transforms.RandomHorizontalFlip(0.5),\n",
    "                  torchvision.transforms.RandomCrop(32, padding = 2),\n",
    "                  torchvision.transforms.ToTensor()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Standardization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put it simple:\n",
    "\n",
    "***normalize***: making your data range in [0, 1]\n",
    "\n",
    "**standardize**: making your data's mean=0 and std=1\n",
    "\n",
    "In modern deep learning, sometimes it's often okay if you don't do these, but they will often help with faster training and better accuracy. Please see this [article](https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean and standard deviation of pixel values so we can standardize the dataset later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.data.float()\n",
    "means = train_dataset.data.mean(axis = (0,1,2)) / 255\n",
    "stds = train_dataset.data.std(axis = (0,1,2)) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the standardization to the list of transformations we want to do.\n",
    "train_transforms.append(torchvision.transforms.Normalize(means, stds))\n",
    "train_transforms = torchvision.transforms.Compose(train_transforms)\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "                           torchvision.transforms.ToTensor(),\n",
    "                           torchvision.transforms.Normalize(mean = means, \n",
    "                                                std = stds)\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomRotation(degrees=[-5.0, 5.0], interpolation=nearest, expand=False, fill=0)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    RandomCrop(size=(32, 32), padding=2)\n",
       "    ToTensor()\n",
       "    Normalize(mean=0.2860405743122101, std=0.35302427411079407)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply these transformations on our training set and testing set separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST(ROOT, \n",
    "                              train = True, \n",
    "                              download = True, \n",
    "                              transform = train_transforms)\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(ROOT, \n",
    "                             train = False, \n",
    "                             download = True, \n",
    "                             transform = test_transforms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave out 10% of data from the training set as the validation set. **The model won't train on the validation set, but only do inference on it.** \n",
    "\n",
    "Validation set is similar to test set (hence the similar transformations), but it's a good practice to only run your model on test set for only **once**, and use your validation set as a gauge of how well your model generalize while tweaking hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALIDATION_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples])\n",
    "\n",
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transform = test_transforms # we do want to do data augmentation on the validation set\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to create a DataLoader object. \n",
    "\n",
    "DataLoader object can be thought of as an iterator we use in Python. Deep learning dataset are usually too large to fit on memory (RAM, usually 8GB to 32GB) entirely, so we want to have a DataLoader that can spit out a fixed size of the dataset every time we need more data to process.\n",
    "\n",
    "Batch_size can be thought of the number of data point we will ask the DataLoader to spit out. After DataLoader spit out a chunk partitioned from the entire dataset, we will send it to GPU's memory (VRAM) so GPU can work on it. Similarly, GPU has limited memory, usually ranging from a few GB to 40GB, so the number should be adjusted according to the VRAM of your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# we only shuffle the training set \n",
    "train_iterator = torch.utils.data.DataLoader(train_data,\n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             shuffle=True)\n",
    "\n",
    "valid_iterator = torch.utils.data.DataLoader(valid_data,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_iterator = torch.utils.data.DataLoader(test_data,\n",
    "                                            batch_size=BATCH_SIZE, \n",
    "                                            shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is defining the model.\n",
    "\n",
    "AlexNet will have the following architecture:\n",
    "\n",
    "* There are 5 2D convolutional layers (which serve as *feature extractors*), followed by 3 linear layers (which serve as the *classifier*).\n",
    "* All layers (except the last one) have `ReLU` activations. (Use `inplace=True` while defining your ReLUs.)\n",
    "* All convolutional filter sizes have kernel size 3 x 3 and padding 1. \n",
    "* Convolutional layer 1 has stride 2. All others have the default stride (1).\n",
    "* Convolutional layers 1,2, and 5 are followed by a 2D maxpool of size 2.\n",
    "* Linear layers 1 and 2 are preceded by Dropouts with Bernoulli parameter 0.5.\n",
    "\n",
    "* For the convolutional layers, the number of channels is set as follows. We start with 3 channels and then proceed like this:\n",
    "\n",
    "  - $3 \\rightarrow 64 \\rightarrow 192 \\rightarrow384 \\rightarrow256 \\rightarrow 256$\n",
    "\n",
    "  In the end, if everything is correct you should get a feature map of size $2\\times2 \\times 256 = 1024$.\n",
    "\n",
    "* For the linear layers, the feature sizes are as follows:\n",
    "\n",
    "  - $1024 \\rightarrow 4096 \\rightarrow 4096 \\rightarrow 10$.\n",
    "\n",
    "  (The 10, of course, is because 10 is the number of classes in FashionMNIST)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot_vision_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28a7dcf41b89ceea79881175bec7b6a3063007774e7dd4410f2273c31f9b85b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
